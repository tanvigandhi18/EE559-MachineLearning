{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvigandhi18/EE559-MachineLearning/blob/master/1_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoHxEsWmqO_V"
      },
      "source": [
        "# EE 508 HW 1 Part 1: Gradient Descent\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XuWmiBbY9Z2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])  #github"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSaMV4aUiE-r"
      },
      "source": [
        "## Gradient Descent for Linear Regression\n",
        "\n",
        "Given two vectors, X and Y, that represent the input and output values of a linear relationship, we can fit a line to this data to predict the output for other values of X. This can be done by finding the parameters $w$ and $b$ of the line, such that the line minimizes the squared error between the predicted and actual output values.\n",
        "\n",
        "The equation for the line is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} = w \\cdot x + b\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\hat{y}$ is the predicted output value.\n",
        "* $w$ is the slope\n",
        "* $b$ is the intercept\n",
        "\n",
        "Let's use Gradient Descent to find the best values for $\\theta$ as defined below:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta = \\begin{bmatrix} w \\\\ b\\end{bmatrix} \\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaEIonqY9aF"
      },
      "source": [
        "## Create Data\n",
        "\n",
        "Let's generate some data with:\n",
        "\n",
        "\\begin{equation} \\theta = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\end{equation}\n",
        "\n",
        "We will use this data to fit the line and then use it to predict the output for other values of $x$.\n",
        "*   `number_of_samples` is an integer variable\n",
        "*   X is a vector of `number_of_samples` numbers between 1, 100\n",
        "*   Y is a vector of `number_of_samples` numbers where each $y = w \\cdot x + b$ plus some noise, which is a random value between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZUKbTJY9aG"
      },
      "outputs": [],
      "source": [
        "number_of_samples = 100\n",
        "\n",
        "X = 2 * np.random.rand(number_of_samples, 1)\n",
        "y = 2 * X + np.random.rand(number_of_samples, 1) + 3\n",
        "\n",
        "theta_guess = np.array([[1.0], [1.0]])\n",
        "learning_rate = 0.001\n",
        "num_iterations = 500\n",
        "\n",
        "final_cost = {}\n",
        "final_theta = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySi6YTFiY9aL"
      },
      "source": [
        "Plot and visualize your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGwW_tP3Y9aM"
      },
      "outputs": [],
      "source": [
        "plt.plot(X,y,'b.')\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "_ = plt.axis([0,np.max(X),0,np.max(y)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORr_MxoXY9al"
      },
      "source": [
        "## Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F0hkmH2Y9am"
      },
      "source": [
        "The equation for calculating cost function and gradients are as shown below.\n",
        "\n",
        "Make prediction:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} =w \\cdot x + b\n",
        "\\end{equation}\n",
        "\n",
        "Mean squared error:\n",
        "\n",
        "\\begin{equation}\n",
        "J(w, b) = \\text{MSE}(w, b) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (e_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "where residul error is\n",
        "\\begin{equation}\n",
        "e_i=\\hat{y}_i - y_i\n",
        "\\end{equation}\n",
        "\n",
        "Gradient:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla J(w, b) = \\left( \\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "Apply chain rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla J(w, b) = \\left( \\sum_{i=1}^n (\\frac{\\partial J}{\\partial e_i} \\cdot \\frac{\\partial e_i}{\\partial w}), \\sum_{i=1}^n(\\frac{\\partial J}{\\partial e_i} \\cdot \\frac{\\partial e_i}{\\partial b}) \\right)\n",
        "\n",
        "= \\left( \\frac{2}{n} \\sum_{i=1}^n e_i \\times {x}_i, \\frac{2}{n} \\sum_{i=1}^n e_i \\times 1 \\right)\n",
        "\\end{equation}\n",
        "\n",
        "Replace $e_i$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla J(w, b) = \\left( \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\times {x}_i, \\frac{2}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\times 1 \\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS9stGjByN90"
      },
      "source": [
        "### **TODO 1:**\n",
        "If we define the residual error as $(y_i - \\hat{y}_i)$, will the graident equation $\\nabla J(w, b)$ change? Show your derive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqJ-szCtyN90"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RDfLeM49EjM"
      },
      "source": [
        "### **TODO 2:**\n",
        "\n",
        "In the next cell, write the Python code for the following functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdAHnLQKY9ao"
      },
      "outputs": [],
      "source": [
        "def calculate_prediction_residuals(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the predictions and residuals for a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,).\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "     Each row represents a sample, and each column represents a feature.\n",
        "                         The last column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[feature_1_sample_1, feature_2_sample_1, 1],\n",
        "                          [feature_1_sample_2, feature_2_sample_2, 1],\n",
        "                          ...\n",
        "                          [feature_1_sample_n, feature_2_sample_n, 1]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "\n",
        "    Returns:\n",
        "    - predictions (numpy.ndarray): Model predictions of shape (num_samples,).\n",
        "    - residuals (numpy.ndarray): Model residuals of shape (num_samples,).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return predictions, residuals\n",
        "\n",
        "\n",
        "def calculate_cost(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculate the cost (Mean Squared Error) for a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - theta (numpy.ndarray): Parameter vector of shape (num_features,)\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,)\n",
        "\n",
        "    Returns:\n",
        "    - cost (float): Computed cost value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return cost\n",
        "\n",
        "def calculate_gradient(X, residuals):\n",
        "    \"\"\"\n",
        "    Calculate the gradient for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features)\n",
        "    - residuals (numpy.ndarray): Residuals (difference between predictions and target values) of shape (num_samples,).\n",
        "\n",
        "    Returns:\n",
        "    - gradient (numpy.ndarray): Gradient vector of shape (num_features,).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ8hQ0MB9rpR"
      },
      "source": [
        "### **TODO 3:**\n",
        "In the next cell, write the Python code to implement Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdrOopKWBonL"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, theta, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The last column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[feature_1_sample_1, feature_2_sample_1, 1],\n",
        "                          [feature_1_sample_2, feature_2_sample_2, 1],\n",
        "                          ...\n",
        "                          [feature_1_sample_n, feature_2_sample_n, 1]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    - theta_history (list): List of parameter vectors over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return theta, cost_history, theta_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hZ9kXyUY9ax"
      },
      "source": [
        "## Gradient Descent Virtualization\n",
        "\n",
        "In the next cell, run the GD implementation, store the cost for each iteration and plot the cost vs. iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh60XqBnY9ax"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add a column of ones to the feature matrix X:\n",
        "X_b = np.c_[X, np.ones((len(X), 1))]\n",
        "\n",
        "# Run Gradient Descent:\n",
        "theta, cost_history, theta_history = gradient_descent(\n",
        "    X_b, y, theta, learning_rate, num_iterations\n",
        ")\n",
        "final_theta['GD']= theta\n",
        "\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['GD']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(num_iterations),cost_history,'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0CVFwvtY9bW"
      },
      "source": [
        "To have a better understanding of how gradient descent actually works, we build a function that virtualizes the training dynamics for different iterations and learning rates combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u71DHEAeY9bZ"
      },
      "outputs": [],
      "source": [
        "def plot_gradient_descent(\n",
        "    num_iterations, learning_rate, X_b, y, data_axis, cost_axis=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize gradient descent optimization for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - X_b (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - data_axis (matplotlib Axis): Axis to visualize the data points and regression lines.\n",
        "    - cost_axis (matplotlib Axis): Optional axis to visualize the cost history vs. iterations.\n",
        "\n",
        "    \"\"\"\n",
        "    # Plot the data points as blue dots\n",
        "    data_axis.plot(\n",
        "        # Assuming the second column is for the intercept term, select the\n",
        "        # first column.\n",
        "        X_b[:, 0], y, \"b.\"\n",
        "    )\n",
        "    # Initialize random theta values from a standard normal distribution\n",
        "    theta = np.random.randn(2, 1)\n",
        "\n",
        "    # Initial transparency value for plotting regression lines\n",
        "    transparency = 0.05\n",
        "\n",
        "    # Initialize an array to store cost history during iterations\n",
        "    cost_history = np.zeros(num_iterations)\n",
        "\n",
        "    # Loop through each iteration\n",
        "    for i in range(num_iterations):\n",
        "        # Compute predictions using the current theta values\n",
        "        predictions_prev = X_b.dot(theta)\n",
        "\n",
        "        # Perform one iteration of gradient descent to update theta values\n",
        "        theta, current_cost, _ = gradient_descent(X_b, y, theta, learning_rate, 1)\n",
        "\n",
        "        # Compute predictions using the updated theta values\n",
        "        predictions = X_b.dot(theta)\n",
        "\n",
        "        # Store the cost value after the current iteration\n",
        "        cost_history[i] = current_cost[0]\n",
        "\n",
        "        # Plot the regression line with adjusted transparency\n",
        "        if i % 25 == 0:\n",
        "            data_axis.plot(\n",
        "                X_b[:, 0], predictions, \"r-\", alpha=transparency\n",
        "            )  # Assuming the first column is for the intercept term\n",
        "            if transparency < 0.8:\n",
        "                transparency += 0.15\n",
        "\n",
        "    # If provided, plot cost history vs. iterations\n",
        "    if cost_axis is not None:\n",
        "        cost_axis.plot(range(num_iterations), cost_history, \"b.\")\n",
        "\n",
        "\n",
        "# Create a figure with subplots for visualization\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "# List of iteration and learning rate pairs to iterate through\n",
        "number_of_iteration_learning_rate_pairs = [(1000, 0.001), (500, 0.01), (100, 0.02)]\n",
        "\n",
        "# Initialize the subplot count for positioning\n",
        "subplot_count = 0\n",
        "\n",
        "# Loop through each iteration and learning rate pair\n",
        "for number_of_iterations, learning_rate in number_of_iteration_learning_rate_pairs:\n",
        "    # Increment the subplot count and create data and cost axes\n",
        "    subplot_count += 1\n",
        "    data_axis = fig.add_subplot(4, 2, subplot_count)\n",
        "    subplot_count += 1\n",
        "    cost_axis = fig.add_subplot(4, 2, subplot_count)\n",
        "\n",
        "    # Set titles for the data and cost axes\n",
        "    data_axis.set_title(\"Learning Rate: {}\".format(learning_rate))\n",
        "    cost_axis.set_title(\"Iterations: {}\".format(number_of_iterations))\n",
        "\n",
        "    # Call the function to visualize gradient descent\n",
        "    plot_gradient_descent(\n",
        "        number_of_iterations, learning_rate, X_b, y, data_axis, cost_axis\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBx-2txOY9cM"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "The main difference between SGD and GD is that SGD uses a **single training example** to update the model parameters at each iteration, while GD uses all of the training examples.\n",
        "\n",
        "\n",
        "* SGD is less computationally expensive than GD and can converge faster. However, SGD can be more noisy than GD, which means that it may not always converge to the global minimum of the function.\n",
        "* GD is more robust to outliers and is less likely to get stuck in local minima. However, GD can be more computationally expensive than SGD and may take longer to converge.\n",
        "\n",
        "\n",
        "### **TODO 4:**\n",
        "In the next cell, write a function to implement SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VcRoNfY9ck"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The last column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[feature_1_sample_1, feature_2_sample_1, 1],\n",
        "                          [feature_1_sample_2, feature_2_sample_2, 1],\n",
        "                          ...\n",
        "                          [feature_1_sample_n, feature_2_sample_n, 1]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return theta, cost_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M14WZu56v1vR"
      },
      "source": [
        "Run your SGD implementation and plot the cost vs iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq2aMN3FY9co"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "\n",
        "X_b = np.c_[X, np.ones((len(X), 1))]\n",
        "\n",
        "theta, cost_history = stochastic_gradient_descent(\n",
        "    X_b, y, theta, learning_rate, num_iterations\n",
        ")\n",
        "final_theta['SGD']= theta\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['SGD']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(num_iterations),cost_history[:num_iterations],'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOyOz_Vh321t"
      },
      "source": [
        "## Stochastic Gradient Descent with Momentum\n",
        "\n",
        "Stochastic Gradient Descent (SGD) with momentum is an optimization technique that incorporates momentum to enhance the convergence of the optimization process. It is particularly useful for training machine learning models, including neural networks. The momentum term helps to smooth out variations in the gradient updates and accelerates convergence along the steepest direction.\n",
        "\n",
        "In SGD with momentum, the update rule for the parameters involves a velocity term that accumulates a fraction of the previous gradients. This velocity term adds inertia to the updates, helping the optimization process to move more consistently and smoothly through the optimization landscape. The formula for the update step is as follows:\n",
        "\n",
        "Velocity Update:\n",
        "\n",
        "\\begin{equation}\n",
        "v_t = \\beta \\cdot v_{t-1} + (1 - \\beta) \\cdot \\nabla J(\\theta_t)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Parameter Update:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{t+1} = \\theta_t - \\alpha \\cdot v_t\n",
        "\\end{equation}\n",
        "\n",
        "* $v_t$ is the velocity at iteration $t$.\n",
        "* $\\beta$ is the momentum parameter between 0 and 1, controlling the retention of previous velocity.\n",
        "* $\\nabla J(\\theta_{t})$ is the gradient of the cost function at iteration $t$ with respect to the parameters $\\theta_{t}$\n",
        "* $\\alpha$ is the learning rate.\n",
        "\n",
        "The velocity term $v_t$ accumulates a fraction of the previous velocity $v_{t-1}$ and adds the current gradient update $\\nabla J(\\theta)$\n",
        " scaled by $(1-\\beta)$. The parameters $\\theta_{t+1}$ are then updated by subtracting the scaled velocity term.\n",
        "\n",
        "### **TODO 5:**\n",
        "In the next cell, implement the SGD with Momentum.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q20nfvlJ38js"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent_with_momentum(X, y, theta, learning_rate=0.01, momentum=0.9, iterations=10):\n",
        "    \"\"\"\n",
        "    Perform stochastic gradient descent with momentum to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The last column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[feature_1_sample_1, feature_2_sample_1, 1],\n",
        "                          [feature_1_sample_2, feature_2_sample_2, 1],\n",
        "                          ...\n",
        "                          [feature_1_sample_n, feature_2_sample_n, 1]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector.\n",
        "    - learning_rate (float): Learning rate for stochastic gradient descent.\n",
        "    - momentum (float): Momentum coefficient.\n",
        "    - iterations (int): Number of iterations for stochastic gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (numpy.ndarray): Array of cost values over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return theta, cost_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E9Ij3pEwJpF"
      },
      "source": [
        "Call your implementaiton of SGD with Momentum and plot the Cost vs iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNntSXOl4CBJ"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "momentum = 0.9\n",
        "\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "X_b = np.c_[X, np.ones((len(X), 1))]\n",
        "theta, cost_history = stochastic_gradient_descent_with_momentum(\n",
        "    X_b, y, theta, learning_rate, momentum, num_iterations\n",
        ")\n",
        "final_theta['SGDM']= theta\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['SGDM']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "_=ax.plot(range(num_iterations),cost_history[:num_iterations],'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6pQsDGvY9cv"
      },
      "source": [
        "# Mini Batch Stochastic Gradient Descent\n",
        "\n",
        "Mini-batch stochastic gradient descent (MBSGD) is a variation of stochastic gradient descent (SGD) where updates are made using small batches of data instead of single examples.\n",
        "\n",
        "This strikes a balance between the efficiency of using the entire dataset (batch gradient descent) and the randomness of SGD.\n",
        "\n",
        "MBSGD computes gradients and updates model parameters in each iteration using a randomly selected mini-batch of data.\n",
        "\n",
        "This approach can lead to faster convergence and better utilization of computational resources compared to traditional SGD or full-batch gradient descent.\n",
        "\n",
        "The size of the mini-batch is a tunable hyperparameter that influences the trade-off between convergence speed and noise in the updates.\n",
        "\n",
        "### **TODO 6:**\n",
        "In the next cell, implement MBSGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQd6KV8oY9cw"
      },
      "outputs": [],
      "source": [
        "def minibatch_gradient_descent(X, y, theta, learning_rate=0.01, num_iterations=10, batch_size=20):\n",
        "    \"\"\"\n",
        "    Perform mini-batch gradient descent to optimize parameters for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Feature matrix of shape (num_samples, num_features).\n",
        "                         Each row represents a sample, and each column represents a feature.\n",
        "                         The last column should be filled with ones for the intercept term.\n",
        "                         Example:\n",
        "                         [[feature_1_sample_1, feature_2_sample_1, 1],\n",
        "                          [feature_1_sample_2, feature_2_sample_2, 1],\n",
        "                          ...\n",
        "                          [feature_1_sample_n, feature_2_sample_n, 1]]\n",
        "    - y (numpy.ndarray): Target values of shape (num_samples,).\n",
        "    - theta (numpy.ndarray): Initial parameter vector of shape (num_features,).\n",
        "    - learning_rate (float): Learning rate for gradient descent.\n",
        "    - num_iterations (int): Number of iterations for gradient descent.\n",
        "    - batch_size (int): Size of each mini-batch.\n",
        "\n",
        "    Returns:\n",
        "    - theta (numpy.ndarray): Optimized parameter vector.\n",
        "    - cost_history (list): List of cost values over iterations.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return theta, cost_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCLVucvxAn3"
      },
      "source": [
        "Run your implementation and plot the cost vs iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erQifW90Y9cy"
      },
      "outputs": [],
      "source": [
        "theta = theta_guess.copy()\n",
        "batch_size = 10\n",
        "\n",
        "print(\"Start:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add a column of ones to the feature matrix X:\n",
        "X_b = np.c_[X, np.ones((len(X), 1))]\n",
        "\n",
        "# Run Gradient Descent:\n",
        "theta, cost_history = minibatch_gradient_descent(\n",
        "    X_b, y, theta, learning_rate, num_iterations, batch_size = batch_size\n",
        ")\n",
        "final_theta['MBGD']= theta\n",
        "\n",
        "print(\"End:\")\n",
        "print(\n",
        "    \"w:          {:0.3f},\\nb:          {:0.3f}\".format(\n",
        "        theta[0][0], theta[1][0]\n",
        "    )\n",
        ")\n",
        "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))\n",
        "final_cost['MBGD']= cost_history[-1]\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax.set_ylabel('J(Theta)')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.plot(range(num_iterations), cost_history,'b.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW-wH0WKhNRn"
      },
      "source": [
        "### **TODO 7:**\n",
        "Answer the following questions in the next cell.\n",
        "\n",
        "For these algorithms: GD, SGD, SGD with momentum, MBGD, answer these questions:\n",
        "\n",
        "1. How would you compare the computation efficiency of each algorithm?\n",
        "2. How would momentum help with the convergence?\n",
        "3. How would different learning rate values affect gradient descent algorithms?\n",
        "4. Does gradient descent always converge to the global optimum?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD8Nm3eTxV3e"
      },
      "source": [
        "Your anwser:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwf91x4pxHDn"
      },
      "source": [
        "## Linear Regression with PyTorch\n",
        "\n",
        "Linear Regression can be implemented using PyTorch by defining a linear regression model, specifying a loss function (usually mean squared error), and using an optimization algorithm (e.g., stochastic gradient descent) to update the model parameters to minimize the loss.\n",
        "\n",
        "### **TODO 8:**\n",
        "In the next cell write a function to perform linear regression using PyTorch. Check this [reference](https://www.geeksforgeeks.org/linear-regression-using-pytorch/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LWJN65sxJZn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_linear_regression(X, y, optimizer_type='SGD', learning_rate=0.01, num_epochs=1000):\n",
        "    \"\"\"\n",
        "    Train a linear regression model using PyTorch.\n",
        "\n",
        "    Parameters:\n",
        "    - X (torch.Tensor): Input data tensor of shape (num_samples, num_features).\n",
        "    - y (torch.Tensor): Target data tensor of shape (num_samples, num_targets).\n",
        "    - optimizer_type (str): Type of optimizer to use. Options: 'SGD', 'Adam', etc.\n",
        "    - learning_rate (float): Learning rate for the optimizer.\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "    - model (nn.Module): Trained linear regression model.\n",
        "    \"\"\"\n",
        "    # Define a linear regression model as a Class and within the constructor\n",
        "    # initilize a linear layer named `linear` instantiated from torch.nn.Linear\n",
        "\n",
        "\n",
        "    # Instantiate the model\n",
        "\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF-xF4MTxO7S"
      },
      "source": [
        "Run your implementation `train_linear_regression`.\n",
        "Plot the trained line on top of the data plot.\n",
        "Print your model parameters $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFeBUXQ9xRy8"
      },
      "outputs": [],
      "source": [
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "trained_model = train_linear_regression(X_tensor, y_tensor, learning_rate=learning_rate, num_epochs=2000)\n",
        "\n",
        "# Print the model parameters\n",
        "weight, bias = trained_model.linear.weight[0].data[0], trained_model.linear.bias.data[0]\n",
        "print(\"weight =\", weight)\n",
        "print(\"bias =\", bias)\n",
        "\n",
        "learned_line = weight * X + bias\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X, learned_line, color='red', label='Learned Line')\n",
        "\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.legend()\n",
        "plt.title('Linear Regression')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "j0CVFwvtY9bW",
        "tuI2x9deY9br"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}