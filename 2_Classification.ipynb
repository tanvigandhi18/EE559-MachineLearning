{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkVvjbLUC-Vp"
      },
      "source": [
        "# EE 508 HW 1 Part 2: Classification\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTo7HGcdC-Vs"
      },
      "source": [
        "## Cross Validation, Bias-Variance trade-off, Overfitting\n",
        "\n",
        "In this section, we will demonstrate data splitting and the validation process in machine learning paradigms. We will use the Iris dataset from the `sklearn` library.\n",
        "\n",
        "Objective:\n",
        "- Train a Fully-Connected Network (FCN) for classification.  \n",
        "- Partition the data using three-fold cross-validation and report the training, validation, and testing accuracy.  \n",
        "- Train the model using cross-entropy loss and evaluate it with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QcYjlGizC-Vt"
      },
      "outputs": [],
      "source": [
        "# import required libraries and dataset\n",
        "import numpy as np\n",
        "# load sklearn for ML functions\n",
        "from sklearn.datasets import load_iris\n",
        "# load torch dataaset for training NNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc_dNmIVC-Vu"
      },
      "source": [
        "### **TODO 1**: Implement the cross validation function\n",
        "In this function, the dataset is first shuffled. Then, we need to implement a loop that iterates through each fold, selecting a subset of samples as the validation set while assigning the remaining samples to the training set, and stores these partitions in the `folds` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oqBdVciCC-Vu"
      },
      "outputs": [],
      "source": [
        "def cross_validation(x: np.array, y: np.array, n_folds: int=3):\n",
        "    \"\"\"\n",
        "    Splitting the dataset to the given fold\n",
        "    Parameters:\n",
        "    - x: Feaures of the dataset, with shape (n_samples, n_features)\n",
        "    - y: Class label of the dataset, with shape (n_samples,)\n",
        "    - n_folds: the given number of partitions\n",
        "        For instnace, 5-fold CV with 100 percentage:\n",
        "        fold_1: training on 20~99, validation on 0~19(%)\n",
        "        fold_2: training on 0~19 and 40~99, validation on 20~39(%)\n",
        "        fold_3: training on 0~39 and 60~99, validation on 40~59(%)\n",
        "        fold_4: training on 0~59 and 80~99, validation on 60~79(%)\n",
        "        fold_5: training on 0~79, validation on 80~99(%)\n",
        "\n",
        "    Returns:\n",
        "    - folds (list): In the format with len(folds) == n_folds\n",
        "        [\n",
        "            (x_train_fold1, y_train_fold1, x_valid_fold1, y_valid_fold1),\n",
        "            (x_train_fold2, y_train_fold2, x_valid_fold2, y_valid_fold2),\n",
        "            (x_train_fold3, y_train_fold3, x_valid_fold3, y_valid_fold3),\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    folds = []\n",
        "    n_data = x.shape[0]\n",
        "    index = np.arange(n_data)\n",
        "    # shaffle the data with np.random.shuffle\n",
        "    np.random.shuffle(index)\n",
        "    # find the partition with numpy.linspace\n",
        "    partitions = np.linspace(0, n_data, num=n_folds+1, endpoint=True)\n",
        "    partitions = partitions.astype(int)\n",
        "\n",
        "    # Finish the code here\n",
        "    # Implementing cross-validation splits\n",
        "    for i in range(n_folds):\n",
        "        valid_idx = index[partitions[i]:partitions[i+1]]\n",
        "        train_idx = np.concatenate((index[:partitions[i]], index[partitions[i+1]:]))\n",
        "\n",
        "        x_train, y_train = x[train_idx], y[train_idx]\n",
        "        x_valid, y_valid = x[valid_idx], y[valid_idx]\n",
        "\n",
        "        folds.append((x_train, y_train, x_valid, y_valid))\n",
        "\n",
        "\n",
        "    print(f\"The Partitions:\")\n",
        "    for idx, (_, train_y, _, valid_y) in enumerate(folds):\n",
        "        print(f\"[Fold-{idx+1}] #Training: {train_y.shape[0]:4>0d}; #Validation: {valid_y.shape[0]:4>0d}\")\n",
        "        from collections import Counter\n",
        "        # you check check the label distribution\n",
        "        print(Counter(train_y))\n",
        "        print(Counter(valid_y))\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BrsG6lBC-Vv",
        "outputId": "f0674176-0f69-4b3d-90c1-0c4f088ec8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Partitions:\n",
            "[Fold-1] #Training: 100; #Validation: 50\n",
            "Counter({1: 35, 2: 34, 0: 31})\n",
            "Counter({0: 19, 2: 16, 1: 15})\n",
            "[Fold-2] #Training: 100; #Validation: 50\n",
            "Counter({2: 35, 1: 33, 0: 32})\n",
            "Counter({0: 18, 1: 17, 2: 15})\n",
            "[Fold-3] #Training: 100; #Validation: 50\n",
            "Counter({0: 37, 1: 32, 2: 31})\n",
            "Counter({2: 19, 1: 18, 0: 13})\n"
          ]
        }
      ],
      "source": [
        "# fixed the random seed\n",
        "np.random.seed(42)\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "# Split into training and testing sets\n",
        "three_folds = cross_validation(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPZeyh5-C-Vv"
      },
      "source": [
        "### **TODO 2**: Build a Fully-Connect Networks with PyTorch\n",
        "In this section, we build simple FCN models with different numbers of hidden units for the classification task.\n",
        "\n",
        "- **Training:** Use cross-entropy for optimization.  \n",
        "- **Inferencing:** Evaluate with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pkBwzk6YC-Vw"
      },
      "outputs": [],
      "source": [
        "# define the FCN model\n",
        "class FCN_model(nn.Module):\n",
        "    # take the argument for the number of hidden units\n",
        "    def __init__(self, n_hidden=32):\n",
        "        # Finish the code here\n",
        "\n",
        "        super(FCN_model, self).__init__()\n",
        "\n",
        "        # Define input and output sizes\n",
        "        n_input = 4  # Number of features in the Iris dataset #Sepal length (cm)Sepal width (cm)Petal length (cm)Petal width (cm)\n",
        "        n_output = 3  # Number of classes in the Iris dataset\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(n_input, n_hidden)  # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU()  # Activation function\n",
        "        self.fc2 = nn.Linear(n_hidden, n_output)  # Hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Finish the code here\n",
        "        x = self.fc1(x)  # First fully connected layer\n",
        "        x = self.relu(x)  # Activation function\n",
        "        x = self.fc2(x)  # Second fully connected layer (output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaEUCC-VC-Vw"
      },
      "source": [
        "Set up the evaluation and training functions for the FCN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ELzI2U0HC-Vx"
      },
      "outputs": [],
      "source": [
        "def eval(model:nn.Module,\n",
        "         x:torch.tensor,\n",
        "         y:torch.tensor) -> float:\n",
        "    \"\"\"Evaluate the model: inference the model with 0/1 loss\n",
        "    We can define the output label is the maximum logit from the model\n",
        "\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x: input features\n",
        "    - y: ground truth labels, dtype=long\n",
        "\n",
        "    Returns:\n",
        "    - loss: the average 0/1 loss value\n",
        "    \"\"\"\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(model(x), dim=1)\n",
        "\n",
        "    loss = 0\n",
        "    # Finish the code here\n",
        "    loss = torch.sum(preds != y).item()\n",
        "\n",
        "    print(f\"Averaging 0/1 loss: {loss/preds.shape[0]:.4f}\")\n",
        "    return loss/preds.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0XQe3EMbC-Vx"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module,\n",
        "          x_train:torch.tensor,\n",
        "          y_train:torch.tensor,\n",
        "          x_valid:torch.tensor,\n",
        "          y_valid:torch.tensor,\n",
        "          epochs:int=300):\n",
        "    \"\"\"Trining process\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x_train, y_train: trainig features and labels (dtype=long)\n",
        "    - x_valid, y_valid: validation features and labels (dtype=long)\n",
        "    - epochs: number of the epoches for training\n",
        "    \"\"\"\n",
        "    # To simplify the process\n",
        "    # we do not take batches but use all the training samples\n",
        "    # set up the objective function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # Forward pass\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {loss.item():.4f}\")\n",
        "            print(f\"[Train] \", end=\"\")\n",
        "            eval(model, x_train, y_train)\n",
        "            print(f\"[Valid] \", end=\"\")\n",
        "            eval(model, x_valid, y_valid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTi8T7RC-Vy"
      },
      "source": [
        "### **TODO 3**: Conduct the training/validation process in each fold\n",
        "We will use three-fold validation, meaning you need to train three models and report the training and validation loss for all three folds.  \n",
        "\n",
        "First, instantiate an FCN model with 32 hidden units.  \n",
        "Then, call the `train` function, which takes the training and validation folds created by the `cross_validation()` function, along with the model, as input. Set `epochs` to `500`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrN9Va7OC-Vy",
        "outputId": "f76bcff6-a97b-44fb-e7f3-ba5141d2a7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6390\n",
            "[Train] Averaging 0/1 loss: 0.3300\n",
            "[Valid] Averaging 0/1 loss: 0.3000\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4884\n",
            "[Train] Averaging 0/1 loss: 0.0900\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.4100\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3528\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [500/500], Cross Entropy Loss: 0.3066\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0300\n",
            "Averaging 0/1 loss: 0.0200\n",
            "===== Traing Fold 1 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6014\n",
            "[Train] Averaging 0/1 loss: 0.3100\n",
            "[Valid] Averaging 0/1 loss: 0.3200\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4617\n",
            "[Train] Averaging 0/1 loss: 0.1100\n",
            "[Valid] Averaging 0/1 loss: 0.1600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.3889\n",
            "[Train] Averaging 0/1 loss: 0.0600\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3358\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [500/500], Cross Entropy Loss: 0.2923\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0800\n",
            "===== Traing Fold 2 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.6292\n",
            "[Train] Averaging 0/1 loss: 0.3200\n",
            "[Valid] Averaging 0/1 loss: 0.3600\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4792\n",
            "[Train] Averaging 0/1 loss: 0.1400\n",
            "[Valid] Averaging 0/1 loss: 0.1200\n",
            "Epoch [300/500], Cross Entropy Loss: 0.4059\n",
            "[Train] Averaging 0/1 loss: 0.0600\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Epoch [400/500], Cross Entropy Loss: 0.3540\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [500/500], Cross Entropy Loss: 0.3106\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0200\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "    # Instantiate a new model for each fold\n",
        "    model = FCN_model(n_hidden=32)\n",
        "\n",
        "    # Define the criterion and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "    # Train the model\n",
        "    train(model, x_train, y_train, x_valid, y_valid, epochs=500)\n",
        "\n",
        "    train_losses.append(eval(model, x_train, y_train))\n",
        "    valid_losses.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2jMo3EC-Vz",
        "outputId": "6996609e-3303-4a46-f84c-2da504566710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.03,            0.02\n",
            "    1,          0.02,            0.08\n",
            "    2,          0.04,            0.02\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqjmTzfC-Vz"
      },
      "source": [
        "### **TODO4**: Check over-fitting with complex model\n",
        "We can follow the same procedure with a more complex FCN model.  \n",
        "Now, set the `number of hidden units` to `2048` and repeat the process for three-fold validation with `epochs = 500`.  \n",
        "The gap between the training and validation performance should increase.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf-_1tPuC-V0",
        "outputId": "c420ce66-ec4a-466f-93b0-d290b316b973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Averaging 0/1 loss: 0.0100\n",
            "Averaging 0/1 loss: 0.0200\n",
            "===== Traing Fold 1 =====\n",
            "Averaging 0/1 loss: 0.0000\n",
            "Averaging 0/1 loss: 0.0600\n",
            "===== Traing Fold 2 =====\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "train_overfit, valid_overfit = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "    # Define the complex model with 2048 hidden units\n",
        "    model = FCN_model(n_hidden=2048)\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
        "\n",
        "    # Training the model\n",
        "    epochs = 500\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = model(x_train)  # Forward pass\n",
        "        loss = criterion(outputs, y_train)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "\n",
        "    train_overfit.append(eval(model, x_train, y_train))\n",
        "    valid_overfit.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ro5ifnlC-V0",
        "outputId": "94d65f3e-add9-4714-871f-741704d7f908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.01,            0.02\n",
            "    1,          0.00,            0.06\n",
            "    2,          0.02,            0.00\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_overfit, valid_overfit)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRPWDH-C-V0"
      },
      "source": [
        "### **TODO 5**: Compare the FCN with statistical ML models\n",
        "Here, we will use the Naive Bayes model from the `sklearn` library and perform three-fold validation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QpAI0d7QC-V1"
      },
      "outputs": [],
      "source": [
        "# Load the Naive Bayes classifier from the library\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "train_nb, valid_nb = [], []\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "\n",
        "    # Finish the code here\n",
        "    # Initialize and train the Naïve Bayes model\n",
        "    model = GaussianNB()\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(x_train)\n",
        "    y_valid_pred = model.predict(x_valid)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_acc = np.mean(y_train_pred == y_train)\n",
        "    valid_acc = np.mean(y_valid_pred == y_valid)\n",
        "\n",
        "    train_nb.append(1 - train_acc)\n",
        "    valid_nb.append(1 - valid_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y89ZpNLNC-V1",
        "outputId": "30797e82-e782-41cf-a0d4-60d760b0d30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.05,            0.04\n",
            "    1,          0.02,            0.06\n",
            "    2,          0.04,            0.04\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_nb, valid_nb)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJgLhk3ZC-V1"
      },
      "source": [
        "### **TODO 6**:\n",
        "Answer the following questions in the next cell.  \n",
        "1. What is the the bias-variance trade-off in machine learning?\n",
        "2. How to reduce overfitting and underfitting?\n",
        "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLHuGDEC-V2"
      },
      "source": [
        "Your anwser:\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\"\"\"\n",
        "1. What is the bias-variance trade-off in machine learning?\n",
        "   - The bias-variance trade-off refers to the balance between two sources of error in a machine learning model:\n",
        "     - **Bias**: Error due to overly simplistic assumptions in the model, leading to underfitting.\n",
        "     - **Variance**: Error due to excessive complexity in the model, leading to overfitting.\n",
        "   - A model with high bias makes strong assumptions and fails to capture underlying patterns, while a model with high variance captures noise in the training data.\n",
        "   - The goal is to find an optimal balance where both bias and variance are minimized.\n",
        "\n",
        "2. How to reduce overfitting and underfitting?\n",
        "   - **To reduce overfitting (high variance)**:\n",
        "     - Use more training data\n",
        "     - Apply regularization (L1, L2)\n",
        "     - Use dropout (for neural networks)\n",
        "     - Simplify the model architecture\n",
        "     - Use data augmentation\n",
        "     - Perform cross-validation\n",
        "   - **To reduce underfitting (high bias)**:\n",
        "     - Increase model complexity (add more layers/nodes in a neural network)\n",
        "     - Use better feature engineering\n",
        "     - Train for more epochs\n",
        "     - Reduce regularization strength\n",
        "\n",
        "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?\n",
        "   - **Naive Bayes**:\n",
        "     - **Training**: Computes class priors and likelihoods using simple probabilistic rules.\n",
        "     - **Inference**: Uses Bayes’ theorem to compute posterior probabilities for each class and selects the class with the highest probability.\n",
        "     - **Computation**: Fast and requires only counting and simple arithmetic operations.\n",
        "   - **Fully Connected Neural Network (FCN)**:\n",
        "     - **Training**: Uses backpropagation and gradient descent to adjust weights based on a loss function.\n",
        "     - **Inference**: Passes input data through multiple layers of neurons, applying learned weights and activation functions.\n",
        "     - **Computation**: Computationally expensive due to matrix multiplications and backpropagation.\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jtRhUmZMkvqt"
      },
      "execution_count": 39,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "caption",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}