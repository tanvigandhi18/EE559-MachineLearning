# -*- coding: utf-8 -*-
"""MutiVariate-Linear-Regression_for_student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9CjF1R9yHjv5VfZMPbkHY0M_KbJ_z8K
"""

import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt

"""# 1. Data Analysis and Data Prepropcessing"""

df = pd.read_csv('/content/train_data.csv') # change the file path according to your own code
df_test = pd.read_csv('/content/test_data.csv')

"""## 1.1 Analyze the data

"""

df.describe()

df.head()

"""## 1.2 Extract the Training feature X and ground-truth label Y"""

# .values is needed here to transfer a Pandas Series into numpy array

X_train_unnorm=df[['X1 transaction date','X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores','X5 latitude','X6 longitude']].values
X_test_unnorm = df_test[['X1 transaction date','X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores','X5 latitude','X6 longitude']].values


y_train=df['Y house price of unit area'].values
y_test = df_test['Y house price of unit area'].values

print('there are {} of samples in training data'.format(len(y_train)))
print('there are {} of samples in  test data'.format(len(y_test)))

"""## 1.3 Standardize the data"""

from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
X_train = standard_scaler.fit_transform(X_train_unnorm)
X_test = standard_scaler.transform(X_test_unnorm)

"""## 1.4 Augment the Data"""

'''
Augment the data for both X_train and X_test
'''
####################################
########### Your code here #########
####################################

"""# 2. Problem Solving

## 2.1 Moore-Penrose pseudoinverse Calculation

# Ideal linear regression results

1. first calculate the weight vector by using Moore Penrose Pseudoinverse

2. Code the mean square error (MSE) cost and MAPE value

3. calcuulation MSE and MAPE for both train and test data based on the learned weight and bias
"""

import time
start_time = time.time()
####################################
########### Your code here #########
####################################
end_time = time.time()
print('calculating the Moore Penrose pseudoinverse need {} seconds'.format(end_time-start_time))

MSE_cost_train = 0.0
MAPE_train = 0.0

MSE_cost_test = 0.0
MAPE_test = 0.0

def mse_cost(X,y,w):
  cost = 0.0
  """
    Compute mean square loss function
     Args:
      X (ndarray (N,D+1)): augmented data, m samples and each sample has dimension D+1
      y (ndarray (N,)) :   ground-truth label y
      w (ndarray (D+1,)) : model parameters

    Returns:
      cost (scalar): cost
    """
    #######################################
    ########### Your code here ############
    #######################################
  return cost

def MAPE_value(X,y,w):
  mape = 0.0
  """
    Compute mape function
    Args:
      X (ndarray (N,D+1)): augmented data, m samples and each sample has dimension D+1
      y (ndarray (N,)) :   ground-truth label y
      w (ndarray (D+1,)) : model parameters
    Returns:
      mape (scalar): cost
    """
    #######################################
    ########### Your code here ############
    #######################################
  return mape



print('training mse cost is {}'.format(MSE_cost_train))
print('training MAPE value is {}'.format(MAPE_train))

print('test mse cost is {}'.format(MSE_cost_test))
print('test MAPE value is {}'.format(MAPE_test))

"""## 2.2 Gradient descent (implements minibatch gradient descent variant2 for linear regression)"""

class MultiVariate_Linear_Regression():
  def __init__(self,X_train,y_train):
    '''
    Args:
      X_train: (ndarray (N,D+1)): augmented train data features, which has N samples and each sample has D+1 dimension
      y_train: (ndarry (N,)): train data labels
    '''
    self.X_train = X_train
    self.y_train = y_train
    self.data_length = len(X_train)
    self.data_dimension = X_train.shape[1]
    self.w = np.zeros(self.data_dimension) # the weight of linear regression model


  def fit_train(self,epochs=40,learning_rate=0.01,batch_size=20):
    """
    To implement minibatch gradient variant 2, you need:
    for each epoch
      1. shuffle whole dataset
      2. get the mini batch data
      3. compute the gradient with regard to each sample in the mini batch
      4. Caculated the gradient needed for update weight and bias
      4. apply the gradient descent to the weight (self.w) and bias (self.b)
    5. calculate and restore the mse_loss using the new weight (just for visualization)
    """

    """
    Tips:
    A proper routine for writing the code is: mse_mape_cost() --> gradient_per_sample() --> fit_train() --> predict()
    """

    """
    implements minibatch gradient descent variant2 for linear regression
    Args:
      epochs (scalar): number of total epochs
      learning_rate (scalar): learning rate for updating the weight
      batch_size (scalar)

    Returns:
      J_hist (list): a list containing the mse loss for each epoch
      MAPE_hist (list): a list containing the MAPE for each epoch
    """

    """
    Attention:
    The computation time should only record the gradient descent!!!!
     Other parts, including data reading, plot or MSE/MAPE calculation should not be included.
    """

    J_hist = [] # history of mse_loss, used for plot the learning curve
    MAPE_hist = []  # history of MAPE
    batch_num = int(np.ceil(self.data_length / batch_size))
    for epoch_idx in range(epochs):
      #########################################
      ###############Your code here###########
      #########################################



      # at the end of each epoch, call mse_loss
      J_wb, MAPE = self.mse_loss(X_train,y_train)
      J_hist.append(J_wb)
      MAPE_hist.append(MAPE)
      pass

    return J_hist, MAPE_hist




  def _compute_gradient_per_sample(self,x,y):
    """
    Computes the single-sample gradient for linear regression

    Args:
      x (ndarray (D+1,)): augmented data, a sample with D+1 dimension
      y (scaler) : target values
        w (ndarray (D+1,)) : model parameters (using self.w)

    Returns:
      dj_dw (ndarray (n,)): the gradient of the cost w.r.t. the parameters w.
    """
    pass


  def mse_mape_cost(self,X, y):
    """
    Compute mean square and MAPE loss functions

    Args:
      X (ndarray (N,D+1)): augmented data, m samples and each sample has dimension D+1
      y (ndarray (N,)) : target values
        w (ndarray (D+1,)) : model parameters (using self.w)

    Returns:
      mse_loss (scalar): cost
      MAPE (scalar): mean absolute percentage error
    """

    #########################################
    ###############Your code here###########
    #########################################
    pass

  def predict(self,X_test):
    """
      Compute output prediction for each data point in X_test
      Args:
        X_test (ndarray (M,D+1)): augmented data, M examples and each sample has dimension D+1
      Returns:
        y_pred (ndarray (M,)): prediction
    """
    #########################################
    ###############Your code here###########
    #########################################
    pass

"""### 2.2.2 Model training"""

"""
Tips:
Initialize the class with train data
train the mode by calling fit_train() function
"""
#########################################
###############Your code here###########
#########################################

"""## 2.2.3 Plot the training curve about the MSE cost and MAPE"""

"""
Tips:
Based on the output of fit_train(), plot the figure use plt.plot()
"""
#########################################
###############Your code here###########
#########################################

"""# 3. Prediction"""

"""
Predict the test data by calling predict()
"""
#########################################
###############Your code here###########
#########################################

